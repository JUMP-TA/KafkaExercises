{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building a Kafka Producer for Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create a Kafka producer script that retrieves weather data from the OpenWeatherMap API for a specified location every minute and publishes it to a Kafka topic. This project will help you understand how to integrate external APIs with Kafka for real-time data streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prerequisites\n",
    "2. Setting Up the Environment\n",
    "3. Installing Required Packages\n",
    "4. Obtaining an OpenWeatherMap API Key\n",
    "5. Setting Up Kafka\n",
    "6. Creating the Kafka Producer Script\n",
    "7. Running the Kafka Producer\n",
    "8. Verifying the Produced Messages\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, ensure you have the following:\n",
    "\n",
    "- **Python 3.7 or higher**: Installed on your machine. You can download it from Python's official website.\n",
    "\n",
    "- **Kafka Cluster**: A running Kafka instance. You can set up a local Kafka environment using Confluent Platform or use a managed Kafka service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to create a virtual environment for your project to manage dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a Project Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Create a new directory for the project`\n",
    "\n",
    "`mkdir kafka_weather_producer`\n",
    "\n",
    "`cd kafka_weather_producer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, using Jupyter Notebook cell magic\n",
    "%mkdir kafka_weather_producer\n",
    "%cd kafka_weather_producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize and Active a Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're working in this notebook, you should already have made an environment to run the cell above. Either way, here are the commands for creating a virtual environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Create a virtual environment named 'venv'`\n",
    "\n",
    "`python3 -m venv venv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Activate the virtual environment`\n",
    "\n",
    "`source venv/bin/activate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: On Windows, activate the virtual environment using `venv\\Scripts\\activate`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Installing Required Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary Python packages using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install confluent_kafka requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ensure that `confluent_kafka` and `requests` are listed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtaining an OpenWeatherMap API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sign Up**: Go to OpenWeatherMap Sign Up and create an account.\n",
    "\n",
    "**API Key**: After verifying your email, navigate to the API keys section to retrieve your API key.\n",
    "\n",
    "*Keep your API key secure and do not share it publicly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting Up Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the traditional Kafka setup that relies on ZooKeeper, KRaft mode allows Kafka to handle its own metadata management, simplifying the architecture. In this section, we'll set up Kafka in KRaft mode using Docker Compose, ensuring that all required configurations, including a unique `CLUSTER_ID`, are properly addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Generating a unique `CLUSTER_ID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CLUSTER_ID is essential for Kafka in KRaft mode to uniquely identify the cluster. We'll generate a UUID (Universally Unique Identifier) to serve as the CLUSTER_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a UUID for CLUSTER_ID\n",
    "cluster_id = str(uuid.uuid4())\n",
    "print(f\"Generated CLUSTER_ID: {cluster_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The actual UUID will be different each time you run the code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating the Docker Compose File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a `docker-compose.yml` file that sets up Kafka in KRaft mode using the generated CLUSTER_ID. This configuration ensures that Kafka operates without ZooKeeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker-compose.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    container_name: kafka_kraft\n",
    "    ports:\n",
    "      - \"9092:9092\"  # Kafka listener\n",
    "      - \"9093:9093\"  # Controller listener\n",
    "    environment:\n",
    "      # Unique Cluster ID (Replace with your generated UUID)\n",
    "      CLUSTER_ID: \"549a81cb-fe1e-4453-ba95-8619625cfe10\"\n",
    "      \n",
    "      # KRaft Mode Configuration\n",
    "      KAFKA_NODE_ID: 1\n",
    "      KAFKA_PROCESS_ROLES: broker,controller\n",
    "      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka_kraft:9093\n",
    "      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      \n",
    "      # Topic Configuration\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n",
    "      \n",
    "      # Additional Configuration (Optional)\n",
    "      KAFKA_LOG_DIRS: /var/lib/kafka/data\n",
    "    volumes:\n",
    "      - kafka_data:/var/lib/kafka/data\n",
    "\n",
    "volumes:\n",
    "  kafka_data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**:\n",
    "\n",
    "Replace \"e4eaaaf2-d142-11e1-b3e4-080027620cdd\" with the CLUSTER_ID you generated in Step 5.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Configuration Parameters**:\n",
    "\n",
    "`CLUSTER_ID`: Your unique Kafka cluster identifier.\n",
    "\n",
    "`KAFKA_NODE_ID`: Unique identifier for the Kafka broker within the cluster. Since we're setting up a single broker, this is set to 1.\n",
    "\n",
    "`KAFKA_PROCESS_ROLES`: Defines the roles of the Kafka process. Here, it's set to both broker and controller.\n",
    "\n",
    "`KAFKA_LISTENERS`: Specifies the endpoints for clients and controllers.\n",
    "\n",
    "`KAFKA_ADVERTISED_LISTENERS`: Advertises the listener to clients.\n",
    "\n",
    "`KAFKA_CONTROLLER_QUORUM_VOTERS`: Defines the controllers in the quorum. Format: <node.id>@<hostname>:<port>.\n",
    "\n",
    "`KAFKA_CONTROLLER_LISTENER_NAMES`: Specifies which listener the controllers use.\n",
    "\n",
    "`KAFKA_INTER_BROKER_LISTENER_NAME`: Defines which listener brokers use to communicate with each other.\n",
    "\n",
    "`volumes`: Mounts a Docker volume to persist Kafka data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Starting the Kafka Broker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `docker-compose.yml` configured, we'll start the Kafka broker in KRaft mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Verifying Kafka is Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Checking Kafka Logs for Successful Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that Kafka has initialized correctly in KRaft mode with the provided CLUSTER_ID, inspect the container logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# You'll need to stop this cell manually once your confirm initialization is complete\n",
    "docker logs -f kafka_kraft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Creating the Kafka Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a Kafka topic named weather to which our producer will send messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec kafka_kraft kafka-topics --create --topic weather --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Verifying Topic Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the weather topic has been successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker exec kafka_kraft kafka-topics --list --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should see weather listed among the topics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Creating the Kafka Producer Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a Python script named weather_producer.py that fetches weather data and sends it to a Kafka topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Define Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_producer.py\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# Configuration Parameters\n",
    "\n",
    "# OpenWeatherMap API Configuration\n",
    "OPENWEATHERMAP_API_KEY = 'removed_for_security'         # Replace with your API key\n",
    "CITY_NAME = 'London'                                    # Replace with your city\n",
    "COUNTRY_CODE = 'UK'                                     # Replace with your country code\n",
    "UNITS = 'metric'                                        # 'metric' or 'imperial'\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'  # Replace if different\n",
    "KAFKA_TOPIC = 'weather'                      # Kafka topic name\n",
    "\n",
    "# OpenWeatherMap API Endpoint\n",
    "OWM_ENDPOINT = 'https://api.openweathermap.org/data/2.5/weather'\n",
    "\n",
    "# Fetch Interval (in seconds)\n",
    "FETCH_INTERVAL = 600  # 600 seconds = 10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(api_key, city, country, units='metric'):\n",
    "    \"\"\"\n",
    "    Fetches weather data from OpenWeatherMap API for the specified location.\n",
    "    \n",
    "    :param api_key: API key for OpenWeatherMap\n",
    "    :param city: City name\n",
    "    :param country: Country code (e.g., 'UK')\n",
    "    :param units: Units of measurement ('metric' or 'imperial')\n",
    "    :return: Dictionary containing weather data or None if failed\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'q': f'{city},{country}',\n",
    "        'appid': api_key,\n",
    "        'units': units\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(OWM_ENDPOINT, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract desired weather stats\n",
    "        weather = {\n",
    "            'city': data.get('name'),\n",
    "            'country': data.get('sys', {}).get('country'),\n",
    "            'temperature': data.get('main', {}).get('temp'),\n",
    "            'humidity': data.get('main', {}).get('humidity'),\n",
    "            'weather_description': data.get('weather', [{}])[0].get('description'),\n",
    "            'timestamp': data.get('dt')  # Unix timestamp\n",
    "        }\n",
    "        return weather\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERROR] Failed to fetch weather data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Delivery Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_callback(err, msg):\n",
    "    \"\"\"\n",
    "    Callback function called once for each message produced to indicate delivery result.\n",
    "    \n",
    "    :param err: Error information, if any\n",
    "    :param msg: The message produced\n",
    "    \"\"\"\n",
    "    if err:\n",
    "        print(f\"[ERROR] Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"[SUCCESS] Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Main Producer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Kafka Producer Configuration\n",
    "    config = {\n",
    "        'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,\n",
    "        'acks': 'all'  # Ensure all replicas acknowledge\n",
    "    }\n",
    "\n",
    "    # Create Producer instance\n",
    "    producer = Producer(config)\n",
    "\n",
    "    print(f\"Starting Kafka producer for weather data: {CITY_NAME}, {COUNTRY_CODE}\")\n",
    "    print(f\"Producing to topic '{KAFKA_TOPIC}' every {FETCH_INTERVAL} seconds.\\n\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Fetch weather data\n",
    "            weather_data = get_weather_data(\n",
    "                OPENWEATHERMAP_API_KEY,\n",
    "                CITY_NAME,\n",
    "                COUNTRY_CODE,\n",
    "                UNITS\n",
    "            )\n",
    "            \n",
    "            if weather_data:\n",
    "                # Serialize weather data to JSON string\n",
    "                weather_json = json.dumps(weather_data)\n",
    "                \n",
    "                # Use city name as the key (optional)\n",
    "                key = weather_data['city'].encode('utf-8')\n",
    "                \n",
    "                # Produce message to Kafka\n",
    "                producer.produce(\n",
    "                    topic=KAFKA_TOPIC,\n",
    "                    key=key,\n",
    "                    value=weather_json,\n",
    "                    callback=delivery_callback\n",
    "                )\n",
    "                \n",
    "                # Trigger delivery report callbacks\n",
    "                producer.poll(0)\n",
    "            \n",
    "            # Wait for the specified interval before next fetch\n",
    "            time.sleep(FETCH_INTERVAL)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n[INFO] Producer interrupted by user. Flushing messages...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Flush any remaining messages\n",
    "        producer.flush()\n",
    "        print(\"[INFO] Producer has been shut down.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Running the Kafka Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this all within the notebook, edit the functions above with your `API_KEY`, `CITY_NAME` and `COUNTRY_CODE` (in variable definitions), and ensure that you're using the right address for your Kafka broker. Run the cell below to start producing. If you'd like, you can copy all the above functions (as well as the `main` execution below) into a script and execute that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Verifying the Produced Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the producer running, you can ensure that your producer is successfully sending messages to the Kafka topic. You can consume messages using the Kafka console consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Start a Kafka Console Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command should be executed in your CLI. You'll see the produced messages in the weather topic. OpenWeatherMap updates its weather data every 10 minutes, so you'll need to wait at least that long if you want to see new messages. You can change the duration between requests, but be sure not to exceed 1000 requests in a day, or you'll get charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash command:\n",
    "`docker exec -it kafka_kraft kafka-console-consumer --bootstrap-server localhost:9092 --topic weather --from-beginning`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you've successfully:\n",
    "- Set up a Python environment with necessary dependencies.\n",
    "\n",
    "- Configured and started a Kafka broker in KRaft mode using Docker.\n",
    "\n",
    "- Created a Kafka producer script that fetches weather data from the OpenWeatherMap API every minute.\n",
    "\n",
    "- Published the fetched data to a Kafka topic.\n",
    "\n",
    "- Verified the data flow using a Kafka console consumer.\n",
    "\n",
    "This setup forms the foundation for building real-time data pipelines and streaming applications using Kafka without the complexity of ZooKeeper. You can further enhance this project by:\n",
    "\n",
    "- Adding Multiple Locations: Modify the script to fetch and produce data for multiple cities.\n",
    "\n",
    "- Implementing Error Handling: Enhance the script to handle potential failures gracefully.\n",
    "\n",
    "- Integrating with Other Systems: Use Kafka consumers to process and analyze the weather data in real-time.\n",
    "\n",
    "\n",
    "Feel free to experiment and expand upon this project to suit your learning objectives!\n",
    "\n",
    "Run the cell below to bring your Docker container down -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker compose down "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
